# -*- coding: utf-8 -*-
"""Email Spam Detection Using Python & Machine Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YMwXCxUsA_Qc_xmiqTIDrOSaeWXkgBLv
"""



""""Email Spam Detection Using Python & Machine Learning" (KAGGLE DATASET- https://www.kaggle.com/balaka18/email-spam-classification-dataset-csv)


Email Spam Classification Dataset CSV
CSV file containing spam/not spam information about 5172 emails.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('emails.csv')

df.head()

df.columns

df.describe

df.shape

## Get the duplicates and remove them
df.drop_duplicates(inplace=True)

## Show the new Shape
df.shape

df.corr()

## Show the number of missing ( NAN)

df.isnull().sum()

"""Creating the NB Model"""

X = df.iloc[:,1:3001]
X

Y = df.iloc[:,-1].values
Y

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from sklearn.model_selection import train_test_split
train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size = 0.25)

"""NAIVES BAYES"""

mnb = MultinomialNB(alpha=1.9)         # alpha by default is 1. alpha must always be > 0. 
# alpha is the '1' in the formula for Laplace Smoothing (P(words))
mnb.fit(train_x,train_y)
y_pred1 = mnb.predict(test_x)
print("Accuracy Score for Naive Bayes : ", accuracy_score(y_pred1,test_y))

## Download the stopwords Package
import nltk
nltk.download('stopwords')



""""SUPPORT VECTOR MACHINE"

Support Vector Machine is the most sought after algorithm for classic classification problems. SVMs work on the algorithm of Maximal Margin, i.e, to find the maximum margin or threshold between the support vectors of the two classes (in binary classification). The most effective Support vector machines are the soft maximal margin classifier, that allows one misclassification, i.e, the model starts with low bias(slightly poor performance) to ensure low variance later.
"""

svc = SVC(C=1.0,kernel='rbf',gamma='auto')         
# C here is the regularization parameter. Here, L2 penalty is used(default). It is the inverse of the strength of regularization.
# As C increases, model overfits.
# Kernel here is the radial basis function kernel.
# gamma (only used for rbf kernel) : As gamma increases, model overfits.
svc.fit(train_x,train_y)
y_pred2 = svc.predict(test_x)
print("Accuracy Score for SVC : ", accuracy_score(y_pred2,test_y))

""""Random Forests (Bagging)""""

rfc = RandomForestClassifier(n_estimators=100,criterion='gini')
# n_estimators = No. of trees in the forest
# criterion = basis of making the decision tree split, either on gini impurity('gini'), or on infromation gain('entropy')
rfc.fit(train_x,train_y)
y_pred3 = rfc.predict(test_x)
print("Accuracy Score of Random Forest Classifier : ", accuracy_score(y_pred3,test_y))